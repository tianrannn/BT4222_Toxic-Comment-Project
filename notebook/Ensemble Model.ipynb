{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Text Classification \n",
    "## BT4222 | Group 5\n",
    "- Chen Yiwen (A0159857H)\n",
    "- Dong Han (A0162689M)\n",
    "- Ma Tianran (A0157706Y)\n",
    "- Tanya Ramesh (A0162732J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our group is working towards a model trained using the abundant online comments, to target at future possible abusive comments. \n",
    "\n",
    "The challenge we have is to build a comprehensive multi-headed model which is capable of classifying if the given input is associated with different types of toxicity such as toxic, threats, obscenity, insults, and identity-based hate. \n",
    "\n",
    "Instead of predicting the classes directly, we decided to predict the probability of each type of toxicity for the input comment. This enables us to set a threshold on deciding if the comment is toxic or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There will be 5 parts of this notebook: \n",
    "- Data Preprocessing\n",
    "- Logistic Regression Model\n",
    "- Multinomial Naive Bayes Model\n",
    "- LSTM Model\n",
    "- Test on Reddit Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/matianran/anaconda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#Import required packages and libraries \n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from textblob import TextBlob\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras import initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dropout, Activation, Embedding, Convolution1D, MaxPooling1D, Input, Dense, \\\n",
    "                         BatchNormalization, Flatten, Reshape, Concatenate\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score,log_loss,confusion_matrix,classification_report,roc_curve,auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment on this subject and which are contra...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  00031b1e95af7921  Your vandalism to the Matt Shirvington article...      0   \n",
       "8  00037261f536c51d  Sorry if the word 'nonsense' was offensive to ...      0   \n",
       "9  00040093b2687caa  alignment on this subject and which are contra...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read Training Data\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Submission_ID</th>\n",
       "      <th>Is_Submitter</th>\n",
       "      <th>Author</th>\n",
       "      <th>Time</th>\n",
       "      <th>Upvotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Welcome to r/Coronavirus! We have a very speci...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>True</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>1.583476e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I was in Waikiki where two confirmed cases wer...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>MaplewoodDUDE</td>\n",
       "      <td>1.583478e+09</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I'm a student at Lake Washington School Distrc...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>AutumnSolace1999</td>\n",
       "      <td>1.583476e+09</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>will disney close it’s us parks?</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>morgandiprima</td>\n",
       "      <td>1.583478e+09</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I think we will reach 100k cases today. We hav...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>Nice_Pro_Clicker</td>\n",
       "      <td>1.583477e+09</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Paramedic here. We could see serious staffing ...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>Blueiguana1100</td>\n",
       "      <td>1.583477e+09</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>The people who are not taking this situation s...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>itsnotaboutewe</td>\n",
       "      <td>1.583480e+09</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>What should I do:\\n\\nHuge family reunion plann...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>Boobznow</td>\n",
       "      <td>1.583479e+09</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>[please boost this press conference ](https://...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>TapatioPapi</td>\n",
       "      <td>1.583484e+09</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Here's a real nice story about the gentleman w...</td>\n",
       "      <td>t3_fe9i00</td>\n",
       "      <td>False</td>\n",
       "      <td>atlasrecrd</td>\n",
       "      <td>1.583479e+09</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Comments  \\\n",
       "0           0  Welcome to r/Coronavirus! We have a very speci...   \n",
       "1           1  I was in Waikiki where two confirmed cases wer...   \n",
       "2           2  I'm a student at Lake Washington School Distrc...   \n",
       "3           3                   will disney close it’s us parks?   \n",
       "4           4  I think we will reach 100k cases today. We hav...   \n",
       "5           5  Paramedic here. We could see serious staffing ...   \n",
       "6           6  The people who are not taking this situation s...   \n",
       "7           7  What should I do:\\n\\nHuge family reunion plann...   \n",
       "8           8  [please boost this press conference ](https://...   \n",
       "9           9  Here's a real nice story about the gentleman w...   \n",
       "\n",
       "  Submission_ID  Is_Submitter            Author          Time  Upvotes  \n",
       "0     t3_fe9i00          True     AutoModerator  1.583476e+09        1  \n",
       "1     t3_fe9i00         False     MaplewoodDUDE  1.583478e+09      314  \n",
       "2     t3_fe9i00         False  AutumnSolace1999  1.583476e+09      454  \n",
       "3     t3_fe9i00         False     morgandiprima  1.583478e+09       45  \n",
       "4     t3_fe9i00         False  Nice_Pro_Clicker  1.583477e+09      117  \n",
       "5     t3_fe9i00         False    Blueiguana1100  1.583477e+09      110  \n",
       "6     t3_fe9i00         False    itsnotaboutewe  1.583480e+09      159  \n",
       "7     t3_fe9i00         False          Boobznow  1.583479e+09       99  \n",
       "8     t3_fe9i00         False       TapatioPapi  1.583484e+09       67  \n",
       "9     t3_fe9i00         False        atlasrecrd  1.583479e+09       34  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ready reddit data\n",
    "df_reddit = pd.read_csv('reddit_comments.csv')\n",
    "df_reddit.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Text Cleaning  #\n",
    "##################\n",
    "\n",
    "# Spelling correction is based on Peter Norvig’s “How to Write a Spelling Corrector” as implemented in the pattern \n",
    "# library. It is about 70% accurate \n",
    "def spelling_correction(row):\n",
    "    b = TextBlob(row)\n",
    "#     print(b)\n",
    "    return b.correct()\n",
    "\n",
    "\n",
    "def clean_text(text, remove_stopwords = False):\n",
    "    '''Remove unwanted characters and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace contractions with their longer forms \n",
    "    # A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "    contractions = { \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\", \"couldn't\": \"could not\", \"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \n",
    "    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\", \"oughtn't\": \"ought not\", \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\", \"she'd\": \"she would\", \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\"that'd\": \"that would\", \"that's\": \"that is\", \"there'd\": \"there had\", \"there's\": \"there is\", \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\", \"they're\": \"they are\", \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\", \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\", \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\"}\n",
    "\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Clean the text\n",
    "    # reference: https://medium.com/@sabber/classifying-yelp-review-comments-using-lstm-and-word-embeddings-part-1-eb2275e4066b\n",
    "    # test site: https://regex101.com/\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)   \n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    #remove twitter @username and html link \n",
    "    test = re.sub(r\"@\\w+\", \" \", text)\n",
    "    test = re.sub(r\"https\\w+\", \" \", text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(df, column_name):\n",
    "    df[column_name] = df[column_name].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # spelling correction is extremely slow......took me around 45 mins to run\n",
    "#     df[column_name] = df[column_name].progress_apply(lambda x: spelling_correction(x))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "###########################\n",
    "# Polarity & Subjectivity #\n",
    "###########################  \n",
    "\n",
    "def get_Polarity(row):\n",
    "    blob = TextBlob(row)\n",
    "    Sentiment = blob.sentiment\n",
    "    return Sentiment.polarity\n",
    "\n",
    "def get_Subjectivity(row):\n",
    "    blob = TextBlob(row)\n",
    "    Sentiment = blob.sentiment\n",
    "    return Sentiment.subjectivity\n",
    "\n",
    "\n",
    "def text_sentiment_preprocessing(df, column_name):\n",
    "    #slow - xx mins\n",
    "    df['Text_Polarity'] = df[column_name].apply(lambda x: get_Polarity(x))\n",
    "    #\n",
    "    df['Text_Subjectivity'] = df[column_name].apply(lambda y: get_Subjectivity(y))\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing training data - This will take some time\n",
      "Pre-processing Reddit data - This will take some time\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-processing training data - This will take some time\")\n",
    "text_preprocessing(df_train, 'comment_text')\n",
    "text_sentiment_preprocessing(df_train, 'comment_text')\n",
    "\n",
    "print(\"Pre-processing Reddit data - This will take some time\")\n",
    "text_preprocessing(df_train, 'comment_text')\n",
    "text_sentiment_preprocessing(df_reddit, 'Comments')\n",
    "df_reddit = df_reddit[['Comments', 'Submission_ID', 'Is_Submitter', 'Author','Time', 'Upvotes',\n",
    "                       'Text_Polarity', 'Text_Subjectivity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['obscene','insult','toxic','severe_toxic','identity_hate','threat']\n",
    "X = df_train[\"comment_text\"]\n",
    "y = df_train[labels]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorization of text for Logreg Model\n",
    "vect_logreg = CountVectorizer(lowercase=False,ngram_range=(1, 2))\n",
    "X_train_dtm = vect_logreg.fit_transform(X_train)\n",
    "X_test_dtm = vect_logreg.transform(X_test)\n",
    "\n",
    "logreg = LogisticRegression(C=12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing obscene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matianran/anaconda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/matianran/anaconda/lib/python3.6/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy is 0.9717157706512475\n",
      "Test accuracy is 0.9686912490913193\n",
      "[2.19593689e-02 2.88523602e-02 2.34344210e-06 ... 2.86300092e-02\n",
      " 1.62057066e-03 1.70786437e-02]\n",
      "AUC Score is 0.9602676800009186\n",
      "Processing insult\n",
      "Training accuracy is 0.9641371012216113\n",
      "Test accuracy is 0.9632516982929336\n",
      "[1.58460644e-02 3.97514768e-02 6.83525511e-08 ... 2.75385187e-02\n",
      " 1.63701026e-03 1.31110549e-02]\n",
      "AUC Score is 0.9456756910568773\n",
      "Processing toxic\n",
      "Training accuracy is 0.957394007252795\n",
      "Test accuracy is 0.9484872032687439\n",
      "[2.33293510e-02 4.48076762e-02 1.21089809e-09 ... 1.48895820e-02\n",
      " 2.14870574e-03 1.75738366e-02]\n",
      "AUC Score is 0.9515814155602742\n",
      "Processing severe_toxic\n",
      "Training accuracy is 0.9913100152074734\n",
      "Test accuracy is 0.9897475747624896\n",
      "[4.31060723e-03 8.44677409e-03 3.78169348e-08 ... 4.45409541e-03\n",
      " 1.04654322e-04 2.24689038e-03]\n",
      "AUC Score is 0.9511768395598934\n",
      "Processing identity_hate\n",
      "Training accuracy is 0.9917612259563161\n",
      "Test accuracy is 0.9905747875567142\n",
      "[4.46693767e-03 8.25612250e-03 4.57211124e-07 ... 4.90621360e-03\n",
      " 4.47713076e-04 2.21476339e-03]\n",
      "AUC Score is 0.930857908419569\n",
      "Processing threat\n",
      "Training accuracy is 0.9981533782315881\n",
      "Test accuracy is 0.9967914170405836\n",
      "[2.88017964e-04 4.07771763e-03 2.79185024e-11 ... 3.68358147e-04\n",
      " 3.85832522e-07 1.46094808e-04]\n",
      "AUC Score is 0.9650067367465629\n"
     ]
    }
   ],
   "source": [
    "prob_result = pd.DataFrame(columns = labels)\n",
    "for label in labels:\n",
    "  print('Processing {}'.format(label))\n",
    "  train_class = y_train[label]\n",
    "  test_class = y_test[label]\n",
    "  logreg.fit(X_train_dtm, train_class)\n",
    "  \n",
    "  # compute the training accuracy\n",
    "  y_train_pred = logreg.predict(X_train_dtm)\n",
    "  y_test_pred = logreg.predict(X_test_dtm)\n",
    "  print('Training accuracy is {}'.format(accuracy_score(train_class, y_train_pred)))\n",
    "  print('Test accuracy is {}'.format(accuracy_score(test_class, y_test_pred)))\n",
    "  \n",
    "  # compute the predicted probabilities for X_test_dtm\n",
    "  test_y_prob = logreg.predict_proba(X_test_dtm)[:,1]\n",
    "  prob_result[label] = test_y_prob\n",
    "\n",
    "  #plot ROC AUC \n",
    "  frp,trp,thres = roc_curve(y_test[label],test_y_prob)\n",
    "  auc_val =auc(frp,trp)\n",
    "  print('AUC Score is {}'.format(auc_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit logreg vectorizer to the entire training dataset \n",
    "X_dtm = vect_logreg.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multinomial Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_com = df_train.comment_text\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=10000,\n",
    "    sublinear_tf = True,\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    strip_accents='unicode')     \n",
    "word_vectorizer.fit(train_com)\n",
    "train_word_features = word_vectorizer.transform(train_com)\n",
    "\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(2, 6),\n",
    "    max_features=50000,\n",
    "    strip_accents='unicode')\n",
    "char_vectorizer.fit(train_com)\n",
    "train_char_features = char_vectorizer.transform(train_com)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC CV score for class obscene is 0.9654996509233292\n",
      "Accuracy CV score for class obscene is 0.9622926502597697\n",
      "AUC CV score for class insult is 0.9641766412130134\n",
      "Accuracy CV score for class insult is 0.9585325566308388\n",
      "AUC CV score for class toxic is 0.9559249409835134\n",
      "Accuracy CV score for class toxic is 0.939844950613152\n",
      "AUC CV score for class severe_toxic is 0.9741089481450566\n",
      "Accuracy CV score for class severe_toxic is 0.9847904739185814\n",
      "AUC CV score for class identity_hate is 0.9376362380346741\n",
      "Accuracy CV score for class identity_hate is 0.9858746285400337\n",
      "AUC CV score for class threat is 0.9046999120841743\n",
      "Accuracy CV score for class threat is 0.9964091221936039\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in labels:\n",
    "    train_target = df_train[i]\n",
    "    classifier = MultinomialNB()\n",
    "    cv_score_auc = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='roc_auc'))\n",
    "    scores.append(cv_score_auc)\n",
    "    print('AUC CV score for class {} is {}'.format(i, cv_score_auc))\n",
    "    \n",
    "    cv_score_acc = np.mean(cross_val_score(classifier, train_features, train_target, cv=3, scoring='accuracy'))\n",
    "    scores.append(cv_score_acc)\n",
    "    print('Accuracy CV score for class {} is {}'.format(i, cv_score_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Training Vocabulary: 187531\n",
      "Size of Reddit Vocabulary: 35441 \n",
      "\n",
      "Words of Training Vocabulary appear > 5 times: 43394\n",
      "Words of Reddit Vocabulary appear > 5 times: 6178 \n",
      "\n",
      "Words of Training Vocabulary appear > 10 times: 27170\n",
      "Words of Reddit Vocabulary appear > 10 times: 3524\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "def word_count(df, column_name):\n",
    "    word_counts = {}\n",
    "\n",
    "    for comment in df[column_name]:\n",
    "        for word in comment.split():\n",
    "            if word not in word_counts:\n",
    "                word_counts[word] = 1\n",
    "            else:\n",
    "                word_counts[word] += 1\n",
    "                \n",
    "    greater_than_5 = sum(1 for i in word_counts.values() if i >= 5)\n",
    "    greater_than_10 = sum(1 for i in word_counts.values() if i >= 10)\n",
    "                \n",
    "    return (len(word_counts), greater_than_5, greater_than_10)\n",
    "\n",
    "print(\"Size of Training Vocabulary:\", word_count(df_train, 'comment_text')[0])\n",
    "print(\"Size of Reddit Vocabulary:\", word_count(df_reddit, 'Comments')[0], '\\n')\n",
    "\n",
    "print(\"Words of Training Vocabulary appear > 5 times:\", word_count(df_train, 'comment_text')[1])\n",
    "print(\"Words of Reddit Vocabulary appear > 5 times:\", word_count(df_reddit, 'Comments')[1], '\\n')\n",
    "\n",
    "print(\"Words of Training Vocabulary appear > 10 times:\", word_count(df_train, 'comment_text')[2])\n",
    "print(\"Words of Reddit Vocabulary appear > 10 times:\", word_count(df_reddit, 'Comments')[2])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the comments \n",
    "\n",
    "max_features = 27170\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(df_train[\"comment_text\"]))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(df_train[\"comment_text\"])\n",
    "list_tokenized_reddit = tokenizer.texts_to_sequences(df_reddit[\"Comments\"])\n",
    "\n",
    "# Check the distribution of the number of words in the comments\n",
    "WordCount = [len(comment) for comment in list_tokenized_train]\n",
    "\n",
    "plt.hist(WordCount,bins = np.arange(0,510,10))#[0,50,100,150,200,250,300,350,400])#,450,500,550,600,650,700,750,800,850,900])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Therefore, define maxlen = 200 as most of the comments has <=150 tokenized words\n",
    "max_features=27170\n",
    "maxlen=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "X_train = list_tokenized_train\n",
    "X_test_reddit = list_tokenized_reddit \n",
    "\n",
    "x_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "x_test_reddit = sequence.pad_sequences(X_test_reddit, maxlen=maxlen)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = df_train[list_classes].values\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train_train, x_train_val, y_train_train, y_train_val = train_test_split(x_train, y_train, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 200)\n",
      "(11725, 200)\n",
      "(159571, 6)\n",
      "For validation purpose\n",
      "(127656, 200)\n",
      "(31915, 200)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test_reddit.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('For validation purpose')\n",
    "print(x_train_train.shape)\n",
    "print(x_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe's embeddings\n",
    "glove6b100d = 'glove.6B.100d.txt'\n",
    "\n",
    "embed_size=100\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove6b100d, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/3\n",
      "127656/127656 [==============================] - 432s 3ms/step - loss: 0.0249 - acc: 0.6112 - val_loss: 0.0144 - val_acc: 0.9938\n",
      "Epoch 2/3\n",
      "127656/127656 [==============================] - 417s 3ms/step - loss: 0.0142 - acc: 0.8087 - val_loss: 0.0129 - val_acc: 0.9938\n",
      "Epoch 3/3\n",
      "127656/127656 [==============================] - 405s 3ms/step - loss: 0.0129 - acc: 0.8377 - val_loss: 0.0127 - val_acc: 0.9938\n"
     ]
    }
   ],
   "source": [
    "def train_model(Optimizer, X_train, y_train, X_val, y_val):\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(max_features, embed_size, weights = [embedding_matrix]))\n",
    "   \n",
    "    model.add(LSTM(units=50, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(Dense(25, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(6,  activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    scores = model.fit(X_train, y_train, batch_size=128, epochs=3, validation_data=(X_val, y_val))\n",
    "    \n",
    "    return scores, model\n",
    "\n",
    "RMSprop_score, RMSprop_model = train_model(Optimizer = 'adam', \\\n",
    "                                           X_train=x_train_train, y_train=y_train_train, \\\n",
    "                                           X_val=x_train_val, y_val=y_train_val)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total roc auc score = 0.9781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "y_train_val_pred = RMSprop_model.predict(x_train_val)\n",
    "auc = roc_auc_score(y_train_val, y_train_val_pred)\n",
    "print(\"Total roc auc score = {0:0.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 499s - loss: 0.0252 - acc: 0.7397\n",
      "Epoch 2/3\n",
      " - 1144s - loss: 0.0138 - acc: 0.9619\n",
      "Epoch 3/3\n",
      " - 553s - loss: 0.0127 - acc: 0.9839\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "def train_model(Optimizer, X_train, y_train):\n",
    "      \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(max_features, embed_size, weights = [embedding_matrix]))\n",
    "   \n",
    "    model.add(LSTM(units=50, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(Dense(25, activation=\"relu\"))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(6,  activation=\"sigmoid\"))\n",
    "    \n",
    "    model.compile(loss='mean_squared_error', optimizer=Optimizer, metrics=['accuracy'])\n",
    "    scores = model.fit(X_train, y_train, batch_size=128, epochs=3, verbose = 2)\n",
    "    return scores, model\n",
    "\n",
    "score, model = train_model(Optimizer = 'adam', X_train=x_train , y_train=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test on Reddit Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obscene\n",
      "insult\n",
      "toxic\n",
      "severe_toxic\n",
      "identity_hate\n",
      "threat\n"
     ]
    }
   ],
   "source": [
    "#generate reddit results\n",
    "reddit_dtm = vect_logreg.transform(df_reddit['Comments'])\n",
    "logreg_reddit_results = pd.DataFrame(columns = labels)\n",
    "\n",
    "for label in labels:\n",
    "    print(label)\n",
    "    train_class = y[label]\n",
    "    logreg.fit(X_dtm, train_class)\n",
    "    predicted_result = logreg.predict_proba(reddit_dtm)[:,1]\n",
    "    logreg_reddit_results[label] = predicted_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_com = df_reddit.Comments\n",
    "reddit_word_features = word_vectorizer.transform(reddit_com)\n",
    "reddit_char_features = char_vectorizer.transform(reddit_com)\n",
    "reddit_features = hstack([reddit_word_features, reddit_char_features])\n",
    "\n",
    "nb_reddit_results = pd.DataFrame(columns = labels)\n",
    "for i in labels:\n",
    "    train_target = df_train[i]\n",
    "    classifier.fit(train_features, train_target)\n",
    "    nb_reddit_results[i] = classifier.predict_proba(reddit_features)[:,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_prediction = model.predict(x_test_reddit)\n",
    "lstm_reddit_results = pd.DataFrame(data=reddit_prediction, columns=list_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average the results of outputs from 3 models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reddit_results = pd.DataFrame(columns=labels)\n",
    "final_reddit_results['comment_text'] = df_reddit.Comments\n",
    "for i in labels:\n",
    "    final_reddit_results[i] = (logreg_reddit_results[i] + lstm_reddit_results[i] +nb_reddit_results[i])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obscene</th>\n",
       "      <th>insult</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>threat</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.303851</td>\n",
       "      <td>0.322770</td>\n",
       "      <td>0.106949</td>\n",
       "      <td>0.333375</td>\n",
       "      <td>0.333400</td>\n",
       "      <td>0.333338</td>\n",
       "      <td>Welcome to r/Coronavirus! We have a very speci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137598</td>\n",
       "      <td>0.180709</td>\n",
       "      <td>0.009999</td>\n",
       "      <td>0.333350</td>\n",
       "      <td>0.333559</td>\n",
       "      <td>0.333357</td>\n",
       "      <td>I was in Waikiki where two confirmed cases wer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.324192</td>\n",
       "      <td>0.328475</td>\n",
       "      <td>0.131185</td>\n",
       "      <td>0.333334</td>\n",
       "      <td>0.333368</td>\n",
       "      <td>0.333356</td>\n",
       "      <td>I'm a student at Lake Washington School Distrc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149040</td>\n",
       "      <td>0.121289</td>\n",
       "      <td>0.144809</td>\n",
       "      <td>0.188171</td>\n",
       "      <td>0.158812</td>\n",
       "      <td>0.160980</td>\n",
       "      <td>will disney close it’s us parks?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.187096</td>\n",
       "      <td>0.216534</td>\n",
       "      <td>0.058355</td>\n",
       "      <td>0.334009</td>\n",
       "      <td>0.335247</td>\n",
       "      <td>0.333425</td>\n",
       "      <td>I think we will reach 100k cases today. We hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11720</th>\n",
       "      <td>0.231517</td>\n",
       "      <td>0.249913</td>\n",
       "      <td>0.225688</td>\n",
       "      <td>0.335869</td>\n",
       "      <td>0.343191</td>\n",
       "      <td>0.333787</td>\n",
       "      <td>It hasn’t. It’s been established that false ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11721</th>\n",
       "      <td>0.038381</td>\n",
       "      <td>0.038719</td>\n",
       "      <td>0.054186</td>\n",
       "      <td>0.135140</td>\n",
       "      <td>0.151499</td>\n",
       "      <td>0.163025</td>\n",
       "      <td>its gonne happen in europe to lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11722</th>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.159176</td>\n",
       "      <td>0.166796</td>\n",
       "      <td>0.336016</td>\n",
       "      <td>0.338125</td>\n",
       "      <td>0.332830</td>\n",
       "      <td>Not even \"sick\" per se, just still shedding vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11723</th>\n",
       "      <td>0.213627</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.049032</td>\n",
       "      <td>0.334114</td>\n",
       "      <td>0.334768</td>\n",
       "      <td>0.333473</td>\n",
       "      <td>they don't know if it's misdiagnosis/false neg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11724</th>\n",
       "      <td>0.110355</td>\n",
       "      <td>0.129325</td>\n",
       "      <td>0.063605</td>\n",
       "      <td>0.333280</td>\n",
       "      <td>0.335890</td>\n",
       "      <td>0.332942</td>\n",
       "      <td>Exactly. The guy I was responding to made it s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11725 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        obscene    insult     toxic  severe_toxic  identity_hate    threat  \\\n",
       "0      0.303851  0.322770  0.106949      0.333375       0.333400  0.333338   \n",
       "1      0.137598  0.180709  0.009999      0.333350       0.333559  0.333357   \n",
       "2      0.324192  0.328475  0.131185      0.333334       0.333368  0.333356   \n",
       "3      0.149040  0.121289  0.144809      0.188171       0.158812  0.160980   \n",
       "4      0.187096  0.216534  0.058355      0.334009       0.335247  0.333425   \n",
       "...         ...       ...       ...           ...            ...       ...   \n",
       "11720  0.231517  0.249913  0.225688      0.335869       0.343191  0.333787   \n",
       "11721  0.038381  0.038719  0.054186      0.135140       0.151499  0.163025   \n",
       "11722  0.107698  0.159176  0.166796      0.336016       0.338125  0.332830   \n",
       "11723  0.213627  0.224200  0.049032      0.334114       0.334768  0.333473   \n",
       "11724  0.110355  0.129325  0.063605      0.333280       0.335890  0.332942   \n",
       "\n",
       "                                            comment_text  \n",
       "0      Welcome to r/Coronavirus! We have a very speci...  \n",
       "1      I was in Waikiki where two confirmed cases wer...  \n",
       "2      I'm a student at Lake Washington School Distrc...  \n",
       "3                       will disney close it’s us parks?  \n",
       "4      I think we will reach 100k cases today. We hav...  \n",
       "...                                                  ...  \n",
       "11720  It hasn’t. It’s been established that false ne...  \n",
       "11721                  its gonne happen in europe to lol  \n",
       "11722  Not even \"sick\" per se, just still shedding vi...  \n",
       "11723  they don't know if it's misdiagnosis/false neg...  \n",
       "11724  Exactly. The guy I was responding to made it s...  \n",
       "\n",
       "[11725 rows x 7 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_reddit_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reddit_results.to_csv('final_reddit_result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
